---
# Document encoding: UTF-8
title: "Real-Time Prediction of Online Shoppers Purchasing Intention"
subtitle: "Model: Logit Generalized Linear Model (GLM)"
author: "ImJaviPerez. UPV-EHU"
date: "June, 2021"
output:
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '3'
    number_sections: yes
    fig_caption: yes
  pdf_document:
    keep_tex: yes
    number_sections: yes
    toc: yes
    toc_depth: 3
    fig_caption: yes
    fig_height: 4
header-includes:
- \usepackage{float}          # To insert figures caption
- \floatplacement{figure}{H}  # To insert figures caption
- \usepackage{xcolor}
- \usepackage{framed}
- \usepackage{fancyhdr}
- \pagestyle{fancy}
- \fancyhead[LE,RO]{Online shoppers}
- \fancyfoot{}
- \fancyfoot[LO,RE]{}
- \fancyfoot[LE,RO]{\thepage}
bibliography: ./biblioMaths.bib
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning=FALSE, message = FALSE, tidy.opts=list(width.cutoff=90), tidy=TRUE)
#    include = FALSE prevents code and results from appearing in the finished file. R Markdown still runs the code in the chunk, and the results can be used by other chunks.
#    echo = FALSE prevents code, but not the results from appearing in the finished file. This is a useful way to embed figures.
#    message = FALSE prevents messages that are generated by code from appearing in the finished file.
#    warning = FALSE prevents warnings that are generated by code from appearing in the finished.
#    fig.cap = "..." adds a caption to graphical results.
#    results='asis' = output as-is, i.e., write raw results from R into the output document
#    eval = If FALSE, knitr will not run the code in the code chunk.
#
# The function tidy_source() in the formatR package (Xie, 2015a) will be used to reformat R code (when the chunk option tidy = TRUE)
```

```{r echo=FALSE}
# Remove variables
### rm(list=ls())
```


```{r myLibraries, echo=FALSE, include=FALSE}
# The function tidy_source() in the formatR package (Xie, 2015a) will be used to reformat R code (when the chunk option tidy = TRUE)
library(formatR)

# Libraries
###library(caret)
#library(readr) # Number parser
library(InformationValue)
library(ggplot2)
library(GGally)
library(gridExtra)
library(car)
library(pander) # Show data frame as a table
library(latex2exp) # Parses and converts LaTeX math formulas to R’s plotmath expressions
library(MLmetrics)
```




```{r}
# More info at: 
# https://medium.com/analytics-vidhya/smote-nc-in-ml-categorization-models-fo-imbalanced-datasets-8adbdcf08c25
# https://github.com/feraguilari/dsc-mod-5-project-online-ds-pt-021119/blob/master/student.ipynb
```


# Subject {-}

We have created a logit Generalized Linear Model (GLM) for *Real-Time Prediction of Online Shoppers Purchasing Intention* based on the paper [@sakar2019real]. The data have been downloaded from [@Dua:2019] *University of California Irvine (UCI) Machine Learning Repository*.

# Introduction
The authors of the paper [@sakar2019real] tell us that the dataset consists of feature vectors belonging to 12330 sessions. The dataset was formed so that each session would belong to a different user in a 1-year period to avoid any tendency to a specific campaign, special day, user profile, or period. Moreover, among the 12330 sessions in the dataset, 84.5% (10422) were negative class samples that did not finalize a transaction and the rest (1908) were positive class samples ending with purchasing.

The dataset consists of 10 numerical and 8 categorical attributes. The `Revenue` attribute can be used as the class label. `Administrative`, `Administrative Duration`, `Informational`, `Informational Duration`, `Product Related` and `Product Related Duration` represent the number of different types of pages visited by the visitor in that session and total time spent in each of these page categories. The values of these features are derived from the URL information of the pages visited by the user and updated in real time when a user takes an action, e.g. moving from one page to another. 

The `Bounce Rate`, `Exit Rate` and `Page Value` features represent the metrics measured by "Google Analytics" for each page in the e-commerce site. 
The value of `Bounce Rate` feature for a web page refers to the percentage of visitors who enter the site from that page and then leave ("bounce") without triggering any other requests to the analytics server during that session. 
The value of `Exit Rate` feature for a specific web page is calculated as for all pageviews to the page, the percentage that were the last in the session. 
The `Page Value` feature represents the average value for a web page that a user visited before completing an e-commerce transaction.

The `Special Day` feature indicates the closeness of the site visiting time to a specific special day (e.g. Mother’s Day, Valentine's Day) in which the sessions are more likely to be finalized with transaction. The value of this attribute is determined by considering the dynamics of e-commerce such as the duration between the order date and delivery date. 
For example, for Valentina’s day, this value takes a nonzero value between February 2 and February 12, zero before and after this date unless it is close to another special day, and its maximum value of 1 on February 8. 

The dataset also includes `operating system`, `browser`, `region`, `traffic type`, `visitor type` (as `returning`, `new visitor` or `other`), a boolean value indicating whether the date of the visit is `weekend`, and `month` of the year.

```{r}
features_decription_df <- read.csv(file="features_decription.csv",
                               row.names=NULL,
                               encoding = "UTF-8",
                               colClasses = c("character", "character", "character"))

require(pander)
panderOptions('table.alignment.default','left')
pander(features_decription_df)
# We will not need this variable
rm(features_decription_df)
```

Table: Features description, by [@sakar2019real].



```{r download_data_from_WWW}
# Download file from internet
file_online_shoppers_intention_WWW <- "http://archive.ics.uci.edu/ml/machine-learning-databases/00468/online_shoppers_intention.csv"
# Read file from your HD
file_online_shoppers_intention_HD <- "./data/online_shoppers_intention.csv"
# Select where is your source file
file_data <- file_online_shoppers_intention_WWW

# Read data from http://archive.ics.uci.edu/ml/machine-learning-databases/00468/online_shoppers_intention.csv and copy to a dataframe.
online_shoppers_df <- read.csv(file=file_data,
                               row.names=NULL,
                               encoding = "UTF-8",
                               colClasses = c("integer", "numeric", "integer", "numeric", "integer", "numeric", "numeric", "numeric", "numeric", "numeric", "factor", "factor", "factor", "factor", "factor", "factor", "logical", "logical"))
```

```{r eval=FALSE, echo=FALSE}
# Month is an ordered level feature
levels(online_shoppers_df$Month)
online_shoppers_df$Month <- ordered(online_shoppers_df$Month,
                levels = c("JAN","Feb","Mar","APR","May","June","Jul","Aug","Sep","Oct","Nov","Dec"))
levels(online_shoppers_df$Month)
```

```{r}
# Separate TRUEs and FALSEs
# It will be used by get_training_rows() and by SMOTE_NC()
# Set of rows with Revenue == TRUE
o_s_T_rows <- which(online_shoppers_df$Revenue == TRUE)
# Set of indices with Revenue == FALSE
o_s_F_rows <- which(online_shoppers_df$Revenue == FALSE)

# size of the training partition data
training_partition_size <- 0.7
```

```{r function_get_training_rows}
#
# function: get_training_rows
# input:
#    new_seed: seed for the Random Number Generator to create a randomized data partition and reproducible results
#    training_dataset_size: size of the training partition data
#    kind_of_set: ("imbalanced", "downsample")
# output:
#    vector with indices of the new training partition data

# We use 70% of dataset for training and the rest for validation.
# We must downSample
#

get_training_rows <- function(new_seed, training_dataset_size=training_partition_size, kind_of_set="imbalanced"){
  # Set seed for reproducible results
  set.seed(new_seed)
  # Create partition Revenue == TRUE
  o_s_T_training_rows <- sample(o_s_T_rows, training_dataset_size*length(o_s_T_rows))
  # Create partition Revenue == FALSE
  o_s_F_training_rows <- sample(o_s_F_rows, training_dataset_size*length(o_s_F_rows))
  
  if(kind_of_set == "downsample"){
    # Downsample
    # There are more FALSES than TRUES. So we have to pick as many FALSES as TRUES
    o_s_F_training_down_rows <- sample(o_s_F_training_rows, length(o_s_T_training_rows))
    # Set of downsampled training rows
    o_s_training_rows <- c(o_s_T_training_rows, o_s_F_training_down_rows)
  } else if(kind_of_set == "imbalanced") {
    # imbalanced
    # Set of imbalanced training rows
    o_s_training_rows <- c(o_s_T_training_rows, o_s_F_training_rows)
  }
return(o_s_training_rows)
}
```





# Generalized Linear Model (GLM)

```{r create_mod1_formula, echo=FALSE}
# Model formula including every variable.
# mod1_formula <- paste0("Revenue ~ ", paste(colnames(online_shoppers_df[,-which(names(online_shoppers_df) == "Revenue")]), collapse = " + "))
mod1_formula <- "Revenue~."
```

```{r create_mod1, echo=FALSE}
# Logit model including every variable.
logitMod1 <- glm(mod1_formula, data=online_shoppers_df, family=binomial(link="logit"))
# summary (logitMod1 )
```

```{r create_anova_mod1, echo=FALSE}
# Analysis of variance
anova_mod1 <- anova(logitMod1, test="Chisq")
```

```{r select_mod2_variables, echo=FALSE}
# Select most significant variables
mod2_variables <- row.names(anova_mod1[which(anova_mod1$`Pr(>Chi)` < 0.1),])
```

```{r create_mod2_formula, echo=FALSE}
# 2nd Model formula with most significant variables
mod2_formula <- paste0("Revenue ~ ", paste(mod2_variables, collapse = " + "))
```
```{r create_mod2, echo=FALSE}
# 2nd Model with most significant variables
logitMod2 <- glm(mod2_formula, data=online_shoppers_df, family=binomial(link="logit"))
# summary (logitMod2 )
```

```{r create_anova_mod2, echo=FALSE}
# anova logit model 2
anova_mod2 <- anova(logitMod2, test="Chisq")
```

```{r select_mod3_variables, echo=FALSE}
# Select most statistically significant variables
mod3_signif_variables <- row.names(anova_mod2[which(anova_mod2$`Pr(>Chi)` < 0.1),])
```

```{r uncorrelated_variables, echo=FALSE}
# These variables are correlated: "ProductRelated", "ProductRelated_Duration"
# These variables are correlated too: "BounceRates", "ExitRates"
# Select every uncorrelated variables: numeric and categorical
mod3_uncorrelated_vars <- mod3_signif_variables[-match(c("ProductRelated", "ProductRelated_Duration", "BounceRates", "ExitRates"), mod3_signif_variables)]
# Couples of uncorrelated variables
mod3_1_uncorrelated_vars <- c("ProductRelated", "BounceRates")
mod3_2_uncorrelated_vars <- c("ProductRelated", "ExitRates")
mod3_3_uncorrelated_vars <- c("ProductRelated_Duration", "BounceRates")
mod3_4_uncorrelated_vars <- c("ProductRelated_Duration", "ExitRates")

# Four lists of uncorrelated variables
index_of_variables_mod3_1 <- match(c(mod3_uncorrelated_vars, mod3_1_uncorrelated_vars), names(online_shoppers_df))
index_of_variables_mod3_2 <- match(c(mod3_uncorrelated_vars, mod3_2_uncorrelated_vars), names(online_shoppers_df))
index_of_variables_mod3_3 <- match(c(mod3_uncorrelated_vars, mod3_3_uncorrelated_vars), names(online_shoppers_df))
index_of_variables_mod3_4 <- match(c(mod3_uncorrelated_vars, mod3_4_uncorrelated_vars), names(online_shoppers_df))
```

```{r numeric_uncorrelated_variables, echo=FALSE}
# List of names of numeric uncorrelated variables: "Administrative", "Informational", "PageValues", "SpecialDay"
mod3_uncorrelated_numeric_vars <- names(dplyr::select_if(online_shoppers_df[,mod3_uncorrelated_vars],is.numeric))

# Four lists of uncorrelated numeric variables
index_of_numeric_variables_mod3_1 <- match(c(mod3_uncorrelated_numeric_vars, mod3_1_uncorrelated_vars), names(online_shoppers_df))
index_of_numeric_variables_mod3_2 <- match(c(mod3_uncorrelated_numeric_vars, mod3_2_uncorrelated_vars), names(online_shoppers_df))
index_of_numeric_variables_mod3_3 <- match(c(mod3_uncorrelated_numeric_vars, mod3_3_uncorrelated_vars), names(online_shoppers_df))
index_of_numeric_variables_mod3_4 <- match(c(mod3_uncorrelated_numeric_vars, mod3_4_uncorrelated_vars), names(online_shoppers_df))
```

```{r glm_formulas, echo=FALSE}
# Create glm formula
mod3_1_variables <- c(mod3_uncorrelated_vars, mod3_1_uncorrelated_vars)
mod3_2_variables <- c(mod3_uncorrelated_vars, mod3_2_uncorrelated_vars)
mod3_3_variables <- c(mod3_uncorrelated_vars, mod3_3_uncorrelated_vars)
mod3_4_variables <- c(mod3_uncorrelated_vars, mod3_4_uncorrelated_vars)
mod3_1_formula <- paste0("Revenue ~ ", paste(mod3_1_variables, collapse = " + "))
mod3_2_formula <- paste0("Revenue ~ ", paste(mod3_2_variables, collapse = " + "))
mod3_3_formula <- paste0("Revenue ~ ", paste(mod3_3_variables, collapse = " + "))
mod3_4_formula <- paste0("Revenue ~ ", paste(mod3_4_variables, collapse = " + "))
```

```{r glm_models, echo=FALSE}
# Models: Uncorrelated significant variables
logitMod3_1 <- glm(mod3_1_formula, data=online_shoppers_df, family=binomial(link="logit"))
logitMod3_2 <- glm(mod3_2_formula, data=online_shoppers_df, family=binomial(link="logit"))
logitMod3_3 <- glm(mod3_3_formula, data=online_shoppers_df, family=binomial(link="logit"))
logitMod3_4 <- glm(mod3_4_formula, data=online_shoppers_df, family=binomial(link="logit"))
```

```{r glm_mod3, echo=FALSE}
# Customize model 3
mod3_couple_uncorrelated_vars <- mod3_4_uncorrelated_vars
index_of_variables_mod3 <- index_of_variables_mod3_4
index_of_numeric_variables_mod3 <- index_of_numeric_variables_mod3_4
mod3_variables <- mod3_4_variables
mod3_formula <- mod3_4_formula
logitMod3 <- logitMod3_4

resumenRevenue3 <- summary (logitMod3 )
```

We want to predict whether next customer visiting a web page will be a Revenue or not. We have selected this set of uncorrelated regressors with the most statistical significance:
```{r results='asis', echo=FALSE}
cat(paste0("`", mod3_variables, "`", collapse = ", "), ". ", sep = "")
```
and we have created a Logit Generalized Linear Model (GLM) to do that prediction. Please, find the [appendix](#GLM_fit) to see the whole variables selection proceedings.

# Experiments and Results

The variable `Revenue` is an imbalanced class. We have an
```{r results='asis', echo=FALSE}
class_proportions <- prop.table(table(online_shoppers_df$Revenue))
cat(round(class_proportions[1] * 100, digits = 1))
```
 % of negative class samples (`Revenue = FALSE`) and
```{r results='asis', echo=FALSE}
cat(round(class_proportions[2] * 100, digits = 1))
```
 % of positives (`Revenue = TRUE`) as we can see in next table:
```{r}
require(pander)
# contingency table
pander(table(online_shoppers_df$Revenue))
```

Table: The variable class `Revenue` is imbalanced. Number of negative and positive class samples ending with purchasing (`Revenue = TRUE`).


There are post hoc sampling approaches that can help attenuate the effects of the imbalance during model training, so we are going to use three kind of sampling training data to implement the models [@kuhn2013applied](p.427):

- the original data, which is imbalanced,
- a downsampled training data and 
- an oversampled training data using the Synthetic Minority Oversampling Technique (SMOTE) methodology.

We use
```{r, results='asis', echo=FALSE}
cat(round(training_partition_size * 100))
```
 % of dataset for training and the rest for validation. Moreover, to ensure statistical significance, this procedure is repeated
```{r, results='asis', echo=FALSE}
# Select the number of tests to do
number_of_tests <- 100 # 10 # 20 # 100
cat(number_of_tests)
```
times with random training/validation partitions.
We are going to show four average metrics to test the results of the models: accuracy, sensitivity (true positive rate), specificity (true negative rate) and F1-score.

```{r create_seeds}
# Set the seed to obtain reproducible results 
set.seed(33)
# Create a set of seeds to initialize the random number generator (RNG) in the next steeps to obtain reproducible results
seeds <- sample(x=1:1E6, size=number_of_tests, replace=F)
```

```{r save_environment}
# Save every variable that we will need in SMOTE_NC_create_datasets.R
pattern_vars <- "training_partition_size|o_s_(T|F)_rows|seeds|number_of_tests|online_shoppers_df|mod3_variables"
# File to saved variables
o_s_environment <- file.path(getwd(), "data", "o_s_environment_4_SMOTE.RData")

save(list = ls()[grepl(pattern_vars, ls())], file=o_s_environment)
# Show variables that we will use in SMOTE_NC_create_datasets.R
# ls()[grepl(pattern_vars, ls())]
```
```{r}
# We have to create some balanced training datasets using the SMOTE algorithm.
# The SMOTE algorithm spends a lot of time for it's calculations, thus we have made
# those datasets in an other script and now we load those results.
smote_datasets_file <- "o_s_SMOTE_NC_datasets.RData" 
# File where are saved SMOTE_NC datasets
smote_datasets_file <- file.path(getwd(), "data", "o_s_SMOTE_NC_datasets.RData")
if (file.exists(smote_datasets_file)) {
  load(smote_datasets_file)
}else{
  stop(paste("File", smote_datasets_file, "DO NOT EXISTS. You must create this file using the script SMOTE_NC_create_datasets.R"))
}
#file.exists(smote_datasets_file)
```


```{r predict_values}
# To ensure statistical significance, this procedure is repeated 100 times with random training/validation partitions.
list_of_datasets <- c("imbalanced", "downsample", "smote")
# Save results in a data.frame
glm_results_df <- data.frame(Dataset = rep(NA, length(list_of_datasets)),
                         Accuracy = rep(NA, length(list_of_datasets)),
                         Sensitivity = rep(NA, length(list_of_datasets)),
                         Specificity = rep(NA, length(list_of_datasets)),
                         F1_score = rep(NA, length(list_of_datasets)))
# Initialize vectors
mis_class_error_results <- rep(NA, number_of_tests) # = 1 - Accuracy
# Save metrics one by one
optCutOff_results <- rep(NA, number_of_tests)
sensitivity_results <- rep(NA, number_of_tests)
specificity_results <- rep(NA, number_of_tests)
F1_score_results <- rep(NA, number_of_tests)

# Save the number of NA's for each training dataset and for each metric
glm_num_NA_df <- data.frame(Dataset = rep(NA, length(list_of_datasets)),
                         Accuracy = rep(NA, length(list_of_datasets)),
                         Sensitivity = rep(NA, length(list_of_datasets)),
                         Specificity = rep(NA, length(list_of_datasets)),
                         F1_score = rep(NA, length(list_of_datasets)))

for (n_dataset in 1:length(list_of_datasets)) {
  # Predict Revenue value for every training set
  for (n_seed in 1:number_of_tests) {
    # For each dataset get training dataset
    if(list_of_datasets[n_dataset] != "smote"){
      # Get randomized training rows
      training_rows <- get_training_rows(seeds[n_seed], kind_of_set = list_of_datasets[n_dataset])
      # Set train and test sets
      train_data <- online_shoppers_df[training_rows, c(mod3_variables, "Revenue")]
      test_data <- online_shoppers_df[-training_rows, c(mod3_variables, "Revenue")]
    } else {
      # Set SMOTE train and test datasets
      train_data <- SMOTE_NC_train_data[[n_seed]]
      test_data <- SMOTE_NC_test_data[[n_seed]]
    }
    
    # Create model for the new training set
    logitMod3_train <- glm(mod3_formula, data=train_data, family=binomial(link="logit"))
    
    # R bug-issue ----
    # To avoid the error: "Error in model.frame.default(Terms, newdata, na.action = na.action, xlev = object$xlevels) : factor TrafficType has new levels 12"
    logitMod3_train$xlevels[["TrafficType"]] <-      union(logitMod3_train$xlevels[["TrafficType"]], levels(test_data$TrafficType))
    logitMod3_train$xlevels[["OperatingSystems"]] <- union(logitMod3_train$xlevels[["OperatingSystems"]], levels(test_data$OperatingSystems))
    # End of R bug-issue ----
    
    # Predict values for testing set
    predicted <- predict.glm(object=logitMod3_train, newdata=test_data, type="response") # predicted scores
    require(InformationValue)
    optCutOff <- optimalCutoff(actuals = test_data$Revenue, predictedScores =  predicted)[1]
    # Find misclassified results
    mis_class_error_results[n_seed] <- misClassError(actuals =  test_data$Revenue,
                                                     predictedScores =  predicted,
                                                     threshold = optCutOff)
    # Save optimalCutoff()
    optCutOff_results[n_seed] <- optCutOff
    sensitivity_results[n_seed] <- sensitivity(test_data$Revenue, predicted, threshold = optCutOff)
    specificity_results[n_seed] <- specificity(test_data$Revenue, predicted, threshold = optCutOff)
    require(MLmetrics)
    # Convert predicted into (0, 1)
    pred_0_1 <- ifelse(predicted < optCutOff, 0, 1)
    F1_score_results[n_seed] <- MLmetrics::F1_Score(y_pred = pred_0_1, y_true = as.numeric(test_data$Revenue), positive = "1")
  }
  
  # Save results for this dataset
  glm_results_df$Dataset[n_dataset] <- list_of_datasets[n_dataset]
  glm_results_df$Accuracy[n_dataset] <- mean(1-mis_class_error_results, na.rm = TRUE) * 100
  glm_results_df$Sensitivity[n_dataset] <- mean(sensitivity_results, na.rm = TRUE)
  glm_results_df$Specificity[n_dataset] <- mean(specificity_results, na.rm = TRUE)
  glm_results_df$F1_score[n_dataset] <- mean(F1_score_results, na.rm = TRUE)
  # Save the number of NA's
  glm_num_NA_df$Dataset[n_dataset] <- list_of_datasets[n_dataset]
  glm_num_NA_df$Accuracy[n_dataset] <- sum(is.na(1-mis_class_error_results))
  glm_num_NA_df$Sensitivity[n_dataset] <- sum(is.na(sensitivity_results))
  glm_num_NA_df$Specificity[n_dataset] <- sum(is.na(specificity_results))
  glm_num_NA_df$F1_score[n_dataset] <- sum(is.na(F1_score_results))
}
```


```{r}
# Show results
panderOptions('round',2)
panderOptions('digits',4)
pander(glm_results_df)
```

Table: Average results of the experiments.


```{r, eval=FALSE, echo=FALSE}
# Show these values only for debugging-testing purposes
# Show the number of NA's  for each dataset and each metric.
pander(glm_num_NA_df)
summary(optCutOff_results)
boxplot(optCutOff_results, horizontal = TRUE)
```


# Appendix. How to fit the GLM {#GLM_fit}

Structure of the R's data frame: 
```{r show_data}
# Show data
str(online_shoppers_df)
```


##  GLM 1st model with every variable

Firstly we create a model including every variable.
```{r create_mod1_formula, echo=TRUE, eval=FALSE}
```
```{r}
mod1_formula
```
```{r create_mod1, echo=TRUE, eval=FALSE}
```


### Analysis of variance. Chi-squared ($\chi^2$) test

We create the analysis of variance (`anova`) tables using a $\chi^2$ test, to find out the most significant variables of the logit model.

```{r create_anova_mod1, echo=TRUE, eval=FALSE}
```
```{r}
anova_mod1
```

The most significant variables have a $p$-value < 0.1:
```{r results='asis', echo=FALSE}
# Most significant variables
cat(paste0("`", mod2_variables, "`", collapse = ", "), ".", sep = "")
```

## GLM 2nd Model with most significant variables

2nd Model with most significant variables:
```{r select_mod2_variables, echo=TRUE, eval=FALSE}
```

```{r create_mod2_formula, echo=TRUE, eval=FALSE}
```
```{r}
mod2_formula
```


```{r create_mod2, echo=TRUE, eval=FALSE}
```

### Analysis of variance. $\chi^2$ test

We create the analysis of variance (`anova`) tables using a $\chi^2$ test for this 2nd model, to find out the most significant variables of this new logit model.

```{r create_anova_mod2, echo=TRUE, eval=FALSE}
```
```{r}
anova_mod2
```

```{r select_mod3_variables, echo=TRUE, eval=FALSE}
```

Every variable have a $p-$value < 0.1, so all of them are statistically significant variables: 
```{r results='asis', echo=FALSE}
# Most significant variables
cat(paste0("`", mod3_signif_variables, "`", collapse = ", "), ".", sep = "")
```

### Avoid variables correlation

In order to improve the accuracy of the model we should avoid correlated variables [@pena2017regresion](p. 429-442). We will do next tasks to find collinear variables:

1. Calculate correlation matrix.
2. Calculate Variance Inflation Factors (*VIF*) to identify the degree of multicollinearity of the predictor variables.

#### Correlation matrix
```{r}
require(GGally)
# List of names of numeric variables
names_of_numeric_variables_mod2 <- names(dplyr::select_if(online_shoppers_df[,mod2_variables],is.numeric))

ggcorr(online_shoppers_df[,names_of_numeric_variables_mod2], 
       label = TRUE, 
       #label_alpha = TRUE,
       layout.exp = 1.3,
       label_round = 2,
       hjust = 0.9)
```

There is a high correlation between two couples of variables:

- Cor(`ProductRelated`, `ProductRelated_Duration`) =
```{r results='asis', echo=FALSE}
cat(round(cor(online_shoppers_df$ProductRelated, online_shoppers_df$ProductRelated_Duration), digits = 2))
```
- Cor(`BounceRates`, `ExitRates`) =
```{r results='asis', echo=FALSE}
cat(round(cor(online_shoppers_df$BounceRates, online_shoppers_df$ExitRates), digits = 2))
```

We have to choose only one of these variables: `ProductRelated` or `ProductRelated_Duration`, but never both of them. And we have to choose again between `BounceRates` or `ExitRates`. So we have four options:

```{r results='asis', echo=FALSE}
cat("\n 1. (", paste0("`", mod3_1_uncorrelated_vars, "`", collapse = ", "), ")", sep = "")
cat("\n 2. (", paste0("`", mod3_2_uncorrelated_vars, "`", collapse = ", "), ")", sep = "")
cat("\n 3. (", paste0("`", mod3_3_uncorrelated_vars, "`", collapse = ", "), ")", sep = "")
cat("\n 4. (", paste0("`", mod3_4_uncorrelated_vars, "`", collapse = ", "), ").", sep = "")
```

We have selected: 
```{r results='asis', echo=FALSE}
cat("(", paste0("`", mod3_couple_uncorrelated_vars, "`", collapse = ", "), ").", sep = "")
```

#### Variance Inflation Factors (VIF)

Hence, the set of variables selected for the GLM are:
```{r results='asis', echo=FALSE}
cat(paste0("`", mod3_variables, "`", collapse = ", "), ". ", sep = "")
```
But we have to know whether these variables are collinear testing the Variance Inflation Factors (VIF):

```{r}
require(car)
pander(round(vif(logitMod3), digits = 2))
```

Table: Variance Inflation Factors (VIF)

We can see that every Generalized VIF (GVIF) is smaller than 4, so we can say that there is not collinearity in this set of predictors.


## GLM 3rd Model


```{r uncorrelated_variables, echo=FALSE, eval=FALSE}
```
```{r numeric_uncorrelated_variables, echo=FALSE, eval=FALSE}
```
```{r glm_formulas, echo=FALSE, eval=FALSE}
```
```{r glm_models, echo=FALSE, eval=FALSE}
```
```{r glm_mod3, eco=FALSE, eval=FALSE}
```

In this moment we have chosen en set of uncorrelated and statistically significant variables. Then, the formula of our last GLM is:

```{r}
mod3_formula
```

This is the R function to create the model:

```{r, eval=FALSE, echo=TRUE}
logitMod3 <- glm(mod3_formula, data=train_data, family=binomial(link="logit"))
```


# References{-}
This document uses [@RCoreTeam].
